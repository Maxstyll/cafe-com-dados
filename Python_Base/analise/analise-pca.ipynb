{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0823a27c597f0d06f9bd705b9ae77e8a6ae593c847b1101b2ecdfc0c7c1f3f928",
   "display_name": "Python 3.8.5 64-bit ('python-base-3.8.5': pyenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "823a27c597f0d06f9bd705b9ae77e8a6ae593c847b1101b2ecdfc0c7c1f3f928"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Análise de dados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/application_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "source": [
    "## Origem dos dados\n",
    "https://www.kaggle.com/c/home-credit-default-risk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dicionario do dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"object\").head()"
   ]
  },
  {
   "source": [
    "## Retirar variaveis que não pertence ao estudo do caso (Drop axi=1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "source": [
    "## Identificar variaveis Nulas e preencher com a media da categoria (Nan)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"object\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NAME_TYPE_SUITE\"].fillna(axis=0, method=\"bfill\", inplace=True)\n",
    "df[\"OCCUPATION_TYPE\"].fillna(axis=0, method=\"bfill\", inplace=True)\n",
    "df[\"FONDKAPREMONT_MODE\"].fillna(axis=0, method=\"bfill\", inplace=True)\n",
    "df[\"HOUSETYPE_MODE\"].fillna(axis=0, method=\"bfill\", inplace=True)\n",
    "df[\"WALLSMATERIAL_MODE\"].fillna(axis=0, method=\"bfill\", inplace=True)\n",
    "df[\"EMERGENCYSTATE_MODE\"].fillna(axis=0, method=\"bfill\", inplace=True)\n",
    "df[\"FONDKAPREMONT_MODE\"].fillna(axis=0, method=\"bfill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FONDKAPREMONT_MODE\"].fillna(value=\"reg oper account\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"int64\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"float64\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"AMT_ANNUITY\"].fillna(value=df[\"AMT_ANNUITY\"].mean(), inplace=True)\n",
    "df[\"AMT_GOODS_PRICE\"].fillna(value=df[\"AMT_GOODS_PRICE\"].mean(), inplace=True)\n",
    "df[\"AMT_REQ_CREDIT_BUREAU_DAY\"].fillna(value=df[\"AMT_REQ_CREDIT_BUREAU_DAY\"].mean(), inplace=True)\n",
    "df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"].fillna(value=df[\"AMT_REQ_CREDIT_BUREAU_WEEK\"].mean(), inplace=True)\n",
    "df[\"AMT_REQ_CREDIT_BUREAU_MON\"].fillna(value=df[\"AMT_REQ_CREDIT_BUREAU_MON\"].mean(), inplace=True)\n",
    "df[\"AMT_REQ_CREDIT_BUREAU_QRT\"].fillna(value=df[\"AMT_REQ_CREDIT_BUREAU_QRT\"].mean(), inplace=True)\n",
    "df[\"AMT_REQ_CREDIT_BUREAU_YEAR\"].fillna(value=df[\"AMT_REQ_CREDIT_BUREAU_YEAR\"].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "source": [
    "## Converter variaveis textos/categoricos para numericos/categoricos "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"object\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_numetic(value, lstText):\n",
    "    for index, text in enumerate(lstText):\n",
    "        if text == value:\n",
    "            return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_Coll = [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\", \"NAME_TYPE_SUITE\", \"NAME_INCOME_TYPE\", \"NAME_EDUCATION_TYPE\", \"NAME_FAMILY_STATUS\", \"NAME_HOUSING_TYPE\", \"OCCUPATION_TYPE\", \"WEEKDAY_APPR_PROCESS_START\", \"ORGANIZATION_TYPE\", \"FONDKAPREMONT_MODE\", \"HOUSETYPE_MODE\", \"WALLSMATERIAL_MODE\", \"EMERGENCYSTATE_MODE\"] \n",
    "\n",
    "for col in lst_Coll:\n",
    "    lstText = df[col].unique()\n",
    "    df[\"CAT_\" + col] = [convert_text_to_numetic(text, lstText) for text in df[col].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## Retirar registros que tenha variaveis com outliers (Escor Z)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns = df.select_dtypes(include=\"float64\").columns\n",
    "des_target = df[\"TARGET\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns = df.select_dtypes(include=\"int64\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variavel in name_columns:\n",
    "    df_defaut = []\n",
    "    for value, target in enumerate(des_target):\n",
    "        dados = df[df[\"TARGET\"]==value]\n",
    "        print(f\"Classe: {des_target[value]} - variavel: {variavel}\")\n",
    "\n",
    "        data_mean, data_std = dados[variavel].mean(), dados[variavel].std()\n",
    "        data_min, data_max = dados[variavel].min(), dados[variavel].max()\n",
    "        print(\"Real Min: %.3f Real Max: %.3f\" %  (data_min, data_max))\n",
    "\n",
    "        cut_off = data_std * 2.5\n",
    "        lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "        print(\"Limit Min: %.3f Limit Max: %.3f\" %  (lower, upper))\n",
    "\n",
    "\n",
    "        dataset = dados[dados[variavel] >= lower]\n",
    "        if len(dataset) == 0:\n",
    "            dataset = dados[dados[variavel] <= upper]\n",
    "        else:\n",
    "            dataset = dataset[dataset[variavel] <= upper]\n",
    "\n",
    "\n",
    "        df_defaut.append(dataset)\n",
    "\n",
    "        outliers = pd.concat([dados[dados[variavel] < lower], dados[dados[variavel] > upper]])\n",
    "        print(\"Identfied outliers: %d \\n\" % len(outliers))\n",
    "\n",
    "    df = pd.concat(df_defaut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collections.Counter(df.TARGET))"
   ]
  },
  {
   "source": [
    "## Balancear as categorias (SMOTE)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = df.select_dtypes(include=\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(name, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"TARGET\"].to_numpy()\n",
    "X = df\n",
    "X = X.drop(\"TARGET\", axis=1)\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overSampling = SMOTE()\n",
    "X, y = overSampling.fit_resample(X, y)\n",
    "print(collections.Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_coll = df.columns[1:]\n",
    "df_smote = pd.DataFrame(X, columns=name_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smote.head()"
   ]
  },
  {
   "source": [
    "## Normalizar as variaveis (StandardScaler)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(X_scaled, columns=name_coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "source": [
    "## Identificar as melhores relações de variaveis para a variavel taget (random forest)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['CAT_CODE_GENDER', 'CAT_FLAG_OWN_CAR', 'CAT_FLAG_OWN_REALTY',\n",
    "       'CAT_NAME_TYPE_SUITE', 'CAT_NAME_INCOME_TYPE', 'CAT_NAME_EDUCATION_TYPE',\n",
    "       'CAT_NAME_FAMILY_STATUS', 'CAT_NAME_HOUSING_TYPE', 'CAT_OCCUPATION_TYPE',\n",
    "       'CAT_WEEKDAY_APPR_PROCESS_START', 'CAT_ORGANIZATION_TYPE', 'CAT_FONDKAPREMONT_MODE',\n",
    "       'CAT_HOUSETYPE_MODE', 'CAT_WALLSMATERIAL_MODE', 'CAT_EMERGENCYSTATE_MODE', 'FLAG_DOCUMENT_2',\n",
    "       'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5',\n",
    "       'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8',\n",
    "       'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11',\n",
    "       'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14',\n",
    "       'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "       'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20',\n",
    "       'FLAG_DOCUMENT_21', 'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE',\n",
    "       'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    df_scaled[col] = df_scaled[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns =['CNT_CHILDREN', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
    "       'DAYS_ID_PUBLISH', 'REGION_RATING_CLIENT',\n",
    "       'REGION_RATING_CLIENT_W_CITY', 'HOUR_APPR_PROCESS_START',\n",
    "       'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "       'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',\n",
    "       'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_columns:\n",
    "    df_scaled[col] = df_scaled[col].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = df_scaled.select_dtypes(include=\"float64\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns.extend(float_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', preprocessing.StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('one_hot', OneHotEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier',  RandomForestClassifier(random_state=5, n_estimators=10))])\n",
    "    \n",
    "model = pipe.fit(df_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns = list(pipe.named_steps['preprocessor'].named_transformers_['cat'].named_steps['one_hot'].get_feature_names(input_features=categorical_columns))\n",
    "numeric_features_list = list(numerical_columns)\n",
    "numeric_features_list.extend(onehot_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.explain_weights(pipe.named_steps['classifier'], top=60, feature_names=numeric_features_list)"
   ]
  },
  {
   "source": [
    "## Aplicar o PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=2)\n",
    "#pca.fit(X)\n"
   ]
  }
 ]
}